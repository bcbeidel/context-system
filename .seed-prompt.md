# Ralph Loop: Dewey - Universal Context Optimization Plugin

**Created**: 2026-02-10
**Purpose**: Implement "Dewey" - a universal context optimization plugin compatible with all major CLI LLM providers (Claude Code, Codex, Gemini)

---

## Overview

**"Dewey"** (like the Dewey Decimal System) - A universal context librarian that organizes and optimizes LLM context across providers.

This Ralph Loop task builds a **Universal Context Optimization Plugin** with:

1. **Signal-to-Noise Measurement** - Track which context is actually helpful
2. **Compaction Strategies** - Prevent context accumulation through file organization
3. **Mid-Term Memory Tier** - Staging area for learnings before permanent promotion
4. **Dual CI/CD Feedback Loops**:
   - **Loop 1**: Self-improving (Dewey's own repository)
   - **Loop 2**: Target repository optimization (once installed)

**Multi-Provider Support**: Compatible with Claude Code, OpenAI Codex, Google Gemini CLI tools

**Approach**: Local, transparent, deterministic operations. No servers, no external dependencies - just scripts, data, and visible recommendations.

**Philosophy**:
- "Start Small" - measure baseline, identify problems from data, implement one fix at a time
- **Transparency** - All changes are recommendations visible in reports/PRs, not automated commits
- **Universal** - Works across LLM providers with provider-specific adapters

**Success Criteria**:
- Measurable KPI improvements (context utilization, token efficiency)
- Dual CI/CD loops: self-improving + target repo optimization
- Multi-provider compatibility (Claude Code, Codex, Gemini)
- Transparent recommendations (human reviews before applying)

---

## Implementation Philosophy

**Based on "Quick Start" Research**:

1. **Measure First**: Can't improve what you don't measure - baseline before optimization
2. **One Change at a Time**: Change one thing, validate, then next (no multi-variable confusion)
3. **Quick Wins First**: High-impact, low-effort improvements before complex features
4. **Data-Driven**: Let usage data guide decisions, not intuition
5. **Incremental**: Small weekly improvements compound over time
6. **Fail Fast**: If optimization doesn't help, revert and try something else

**Anti-Pattern**: Don't try to build the full MCP server before measuring baseline usage.
**Better**: Start with token inventory + session tracking, then optimize based on data.

---

## File Structure

```
dewey/                        # Plugin repository (self-improving)
├── pyproject.toml           # Python package config
├── setup.py                 # Extension packaging
├── manifest.yaml            # Claude Code extension manifest
├── .github/
│   └── workflows/
│       └── dewey-ci.yml     # CI/CD Loop 1: Self-improvement
├── src/
│   ├── __init__.py
│   ├── cli/                 # Slash command interface
│   │   ├── commands.py      # Command implementations
│   │   ├── parser.py        # Argument parsing
│   │   └── formatters.py    # Output formatting
│   ├── core/                # Core functionality
│   │   ├── measurement/     # Signal-to-noise tracking
│   │   │   ├── token_counter.py
│   │   │   ├── frequency_tracker.py
│   │   │   └── citation_tracker.py
│   │   ├── compaction/      # File optimization
│   │   │   ├── file_splitter.py
│   │   │   ├── duplicate_detector.py
│   │   │   └── extractive_summarizer.py
│   │   ├── memory/          # Mid-term tier
│   │   │   ├── session_manager.py
│   │   │   └── promotion_engine.py
│   │   └── analytics/       # Usage analysis
│   │       ├── logger.py
│   │       ├── analyzer.py
│   │       └── dashboard_generator.py
│   ├── cicd/                # CI/CD integration
│   │   ├── self_improvement.py    # Loop 1: Dewey's own CI/CD
│   │   ├── target_repo_gates.py   # Loop 2: Target repo CI/CD
│   │   ├── kpi_checker.py
│   │   └── recommendation_engine.py
│   ├── providers/           # Multi-provider adapters
│   │   ├── base.py          # Base adapter interface
│   │   ├── claude.py        # Claude Code adapter
│   │   ├── codex.py         # OpenAI Codex adapter
│   │   └── gemini.py        # Google Gemini adapter
│   ├── extensions/          # Provider-specific extension code
│   │   ├── claude/          # Claude Code extension
│   │   │   ├── manifest.yaml
│   │   │   └── dewey.py
│   │   ├── codex/           # Codex extension
│   │   │   ├── extension.json
│   │   │   └── dewey.py
│   │   └── gemini/          # Gemini plugin
│   │       ├── plugin.yaml
│   │       └── dewey.py
│   └── reports/             # Recommendation generation
│       ├── report_generator.py
│       ├── suggestion_formatter.py
│       └── pr_generator.py  # Creates transparent PR with suggestions
├── tests/                   # Comprehensive test suite
│   ├── test_measurement.py
│   ├── test_compaction.py
│   ├── test_memory.py
│   ├── test_cicd_loops.py
│   ├── test_commands/       # Slash command tests
│   │   ├── test_init.py
│   │   ├── test_check.py
│   │   └── test_report.py
│   └── test_providers/      # Provider compatibility tests
│       ├── test_claude.py
│       ├── test_codex.py
│       └── test_gemini.py
├── scripts/                 # CLI utilities
│   ├── build-extensions.sh  # Build provider-specific extensions
│   ├── publish.sh           # Publish to extension registries
│   ├── track-context.sh
│   ├── analyze-usage.py
│   └── consolidate.sh
└── templates/               # Templates for target repos
    ├── .github/workflows/
    │   └── dewey-checks.yml # CI/CD Loop 2 template
    ├── .dewey/
    │   ├── config.yml       # Target repo configuration
    │   └── analytics/       # Target repo data storage
    └── hooks/
        └── pre-commit       # Context quality pre-commit hook
```

---

## Specifications (specs/)

Create these specification files that define requirements:

### spec-0-quick-start.md

**Foundation: First Week Quick Wins**

**Requirements**:
1. Token inventory script (counts all context files)
2. Session tracking template (manual capture)
3. Baseline measurement report
4. One immediate optimization (split largest file OR archive old files)

**Acceptance Criteria**:
- Complete inventory CSV generated with file, lines, bytes, tokens
- Session template created and documented
- Baseline report captures: total files, total tokens, file size distribution
- One optimization implemented with before/after comparison
- Can run full workflow in <1 hour

**Script Outputs**:
- `~/.claude/analytics/context-inventory.csv`
- `.claude/sessions/template.md`
- `~/.claude/analytics/baseline.txt`

**Success Definition**:
- Know exact baseline (X files, Y tokens)
- Have template for tracking sessions
- One measurable improvement implemented

**Based on**: `analysis/quick-start-guide.md` - "Start Small" philosophy

### spec-1-measurement.md

**Signal-to-Noise Measurement System**

**Requirements**:
1. Token counter that accurately estimates tokens per file (4 chars ≈ 1 token)
2. Frequency tracker that logs which files are loaded per session
3. Citation tracker that detects which loaded files are referenced in responses
4. CSV-based logging (no database dependencies)
5. Weekly analysis scripts generating markdown reports

**Acceptance Criteria**:
- Can generate complete token inventory of context/ directory
- Tracks file loads across sessions with timestamps
- Calculates utilization rate (files cited / files loaded)
- Produces weekly markdown dashboard with metrics
- All deterministic (no LLM calls for core functionality)

**Metrics to Track**:
- Context Utilization: % of available tokens used
- Token Efficiency: Useful output / total tokens
- Information Density: Actionable tokens / total tokens
- Precision@5: Relevant files in top 5 loaded
- Citation Rate: Files referenced / files loaded

### spec-2-compaction.md

**File Compaction and Consolidation**

**Requirements**:
1. Automated file splitting for files >500 lines (main + references/)
2. Duplicate content detection using paragraph hashing
3. Dead link checker for wikilinks
4. Time-based archival (>90 days old for decisions/retros)
5. Extractive summarization using TF-IDF (no LLM)

**Acceptance Criteria**:
- All files <500 lines after processing
- Duplicate paragraphs identified across files
- All wikilinks resolve correctly
- Old files automatically archived with searchable index
- Summaries maintain 90%+ information retention

**Constraints**:
- Preserve file functionality (no breaking changes)
- Maintain git history
- Update all wikilinks when files move
- Create backup before destructive operations

### spec-3-memory-tier.md

**Mid-Term Memory Staging System**

**Requirements**:
1. Session file template for capturing learnings
2. Working context directory (week-based organization)
3. Promotion decision checklist (frequency, scope, validation, clarity)
4. Automated promotion suggestions based on age + reference count
5. Weekly review workflow
6. Conversation summary buffer (hierarchical memory tiers)
7. Session-end hooks for automated capture

**Acceptance Criteria**:
- Session files created automatically with structured sections
- Weekly files track validated patterns vs. emerging patterns
- Promotion checklist has clear pass/fail criteria
- Script suggests high-confidence promotion candidates
- Manual approval required before permanent promotion
- 7-day retention policy for mid-term (aligns with production patterns)
- Graceful degradation (system works without mid-tier)

**Three-Tier Architecture** (based on MIRIX/Memoria patterns):
- **Short-term** (Working Memory): Current session, in-context, ephemeral
- **Mid-term** (Episodic Memory): 7 days, session files, staged learnings
- **Long-term** (Semantic Memory): Permanent, context/ files, validated knowledge

**Promotion Rules** (deterministic):
- Age >7 days + referenced 3+ times → Promote
- Explicitly validated → Promote
- Referenced 5+ times → Promote (regardless of age)
- User says "remember this" → Promote
- Solves recurring problem (3+ occurrences) → Promote
- Score <0.3 → Discard
- Score 0.3-0.6 → Keep in mid-term

**Session Management Patterns**:
- Conversation Buffer Window: Keep last K interactions
- Conversation Summary Buffer: Summarize old, keep recent raw
- Hierarchical Memory: Multiple tiers with different retention policies

### spec-4-analytics.md

**Usage Analytics and Feedback Loop**

**Requirements**:
1. CSV logging of every context load (timestamp, file, tokens, task)
2. Citation tracking via grep-based phrase matching
3. Pandas-based analysis generating statistics
4. Markdown dashboard generator (weekly reports)
5. A/B testing framework for comparing optimization strategies
6. Context attribution (which files contributed to answers)
7. Tracing architecture (complete path from input → output)
8. Feedback loop implementation (measure → analyze → test → deploy)

**Acceptance Criteria**:
- >90% of sessions logged to CSV
- Weekly dashboards generated automatically
- Identifies most/least valuable files
- Calculates utilization improvements over time
- A/B tests have statistical significance checks (p < 0.05)
- Citation rate calculated per file
- Context utilization tracked over time

**Key Metrics** (based on production standards):
- **Context Load Ratio**: 20-40% healthy, >70% problematic
- **Context Reference Rate**: >70% excellent, <50% poor
- **Groundedness**: ≥95% (minimal hallucination)
- **Token Efficiency**: >0.70 target, >0.80 excellent
- **Hallucination Rate**: <5% target, <1% for high-trust

**Dashboard Sections**:
- Overview (sessions, avg files/session, avg tokens, utilization)
- Top loaded files (by frequency)
- Most valuable files (high citation rate)
- Least valuable files (low citation rate)
- Recommendations based on data
- Trend analysis (week-over-week improvements)

**A/B Testing Framework**:
- Variant assignment (hash-based or random)
- Statistical significance testing (t-tests, sample size calculation)
- Guardrail metrics (latency, cost, satisfaction)
- Multi-armed bandit (optional: adaptive traffic allocation)

**Observability Integration** (Phase 4):
- Langfuse (open source) evaluation
- OpenTelemetry tracing
- Automated alerts and dashboards

### spec-5-dual-cicd-loops.md

**Dual CI/CD Feedback Loops with Transparent Recommendations**

**Loop 1: Dewey Self-Improvement** (dewey/.github/workflows/dewey-ci.yml)
- Runs on Dewey's own repository
- Evaluates Dewey's code quality, test coverage, documentation
- Generates PR with improvements to Dewey itself
- Example: "Add support for YAML frontmatter in citation tracker"

**Loop 2: Target Repository Optimization** (templates/.github/workflows/dewey-checks.yml)
- Installs into target repository when user runs `dewey install`
- Evaluates target repo's context quality
- Generates PR with specific recommendations for context improvements
- Example: "Split context/skills/ralph-loop-guide.md into main + references/"

**Requirements**:
1. **Both loops generate PRs** (not direct commits) - transparency is key
2. Quality gates (enforce context standards, fail CI if violated)
3. Recommendation engine (suggests specific, actionable improvements)
4. KPI evaluation script (measures context metrics)
5. Pre-commit hooks (prevent regressions in target repo)
6. Self-improvement detector (identifies patterns in Dewey's own usage for Loop 1)

**Acceptance Criteria**:
- Loop 1: PR created on Dewey repo when self-improvement opportunity detected
- Loop 2: PR created on target repo with context optimization suggestions
- Both PRs include: what to change, why, expected impact, implementation steps
- CI/CD checks fail if KPIs below thresholds
- Quality gates enforce: file size <500 lines, no duplicates, no dead links
- Pre-commit hooks validate file size and link integrity
- User can review and approve/reject all suggestions (full transparency)

**KPI Thresholds** (CI/CD gates):
```yaml
context_utilization: min=0.60  # Warn if <60%
token_efficiency: min=0.70     # Fail if <70%
file_size_max: 500             # Fail if any file >500 lines
duplicate_content_max: 0.05    # Warn if >5% duplicate
dead_links: 0                  # Fail if any dead links
```

**Recommendation Categories**:
1. **File Splitting**: "File X has 633 lines, split into main + references/"
2. **Duplicate Removal**: "Paragraph Y appears in 3 files, consolidate to canonical"
3. **Archive Old Content**: "Files Z1, Z2, Z3 not modified in 90+ days, archive"
4. **Load Optimization**: "File A loaded 90% of time but cited 12%, create summary"
5. **Missing Context**: "Query B failed, missing file C in domain D"

**Loop 1 PR Format** (Dewey self-improvement):
```markdown
# [Dewey Self-Improvement] Add YAML Frontmatter Support

## Detected Pattern
- 15% of target repos use YAML frontmatter
- Citation tracker currently only parses markdown headers
- Missing frontmatter metadata reduces citation accuracy

## Proposed Changes
1. Add YAML parser to `src/core/measurement/citation_tracker.py`
2. Extract keywords from frontmatter `keywords:` field
3. Update tests to cover frontmatter parsing

## Expected Impact
- +8% citation accuracy in repos with frontmatter
- Better compatibility with Obsidian-style vaults

## Files Changed
- `src/core/measurement/citation_tracker.py`
- `tests/test_measurement.py`
```

**Loop 2 PR Format** (Target repo optimization):
```markdown
# [Dewey] Context Optimization Recommendations - Week N

## KPI Summary
- Context Utilization: 68% (target: 70%)
- Token Efficiency: 0.74 (target: 0.70) ✓
- Files >500 lines: 2 (target: 0) ⚠️

## Proposed Changes

### HIGH PRIORITY
1. **Split ralph-loop-guide.md** (633 lines)
   - Impact: -15,000 tokens/load
   - Effort: 30 min
   - Changes:
     ```diff
     + context/skills/ralph-loop-guide.md (150 lines)
     + context/skills/references/ralph-loop/philosophy.md
     + context/skills/references/ralph-loop/patterns.md
     ```

### MEDIUM PRIORITY
2. **Archive old decisions** (12 files >90 days)
   - Impact: -8,000 tokens available context
   - Effort: 15 min
   - Changes:
     ```diff
     - context/decisions/2025-*.md
     + context/archive/2025/decisions/
     ```

## Trends
- Context utilization: 62% → 68% (+6% this week)
- Most improved: skills domain (+12% citation rate)

---
*Generated by Dewey Context Optimizer*
*Review and merge if acceptable, or close if not needed*
```

---

### spec-6-multi-provider.md

**Universal Provider Compatibility**

**Requirements**:
1. Provider adapter interface (base class defining common operations)
2. Claude Code adapter (context/ directory conventions, CLAUDE.md)
3. OpenAI Codex adapter (AGENTS.md, SKILL.md conventions)
4. Google Gemini adapter (Gemini-specific context patterns)
5. Auto-detection (identify provider from repository structure)
6. Provider-specific configuration (customize behavior per provider)
7. Extension installation for each provider
8. Slash command registration (provider-specific mechanisms)

**Acceptance Criteria**:
- Base adapter interface with methods: load_context(), get_config(), validate_structure()
- Each provider adapter implements base interface
- Auto-detection correctly identifies provider 95%+ of time
- Provider-specific quality gates (e.g., Claude: CLAUDE.md required, Codex: AGENTS.md format)
- Works correctly with all three providers in integration tests
- Extension installs via standard mechanisms (Claude Code extensions, etc.)
- Slash commands work in each provider's CLI
- Documentation explains provider differences and customization

**Provider Detection Logic**:
```python
def detect_provider(repo_path):
    """Auto-detect LLM CLI provider from repository structure"""
    if exists(repo_path / ".claude/"):
        return "claude"  # Claude Code
    elif exists(repo_path / "AGENTS.md"):
        return "codex"   # OpenAI Codex
    elif exists(repo_path / ".gemini/"):
        return "gemini"  # Google Gemini
    else:
        return "unknown"  # Prompt user to specify
```

**Provider-Specific Conventions**:

| Aspect | Claude Code | OpenAI Codex | Google Gemini |
|--------|-------------|--------------|---------------|
| **Config location** | `.claude/CLAUDE.md` | `AGENTS.md` + `SKILL.md` | `.gemini/config.yaml` |
| **Context directory** | `context/` | `docs/` or `context/` | `.gemini/context/` |
| **Link format** | `[[wikilinks]]` | `[markdown](links)` | Both supported |
| **Max file size** | 500 lines (progressive disclosure) | 1000 lines | 800 lines |
| **Extension install** | `claude code install dewey` | `codex extensions add dewey` | `gemini plugins install dewey` |
| **Skill/Command location** | `.claude/skills/` | `.codex/commands/` | `.gemini/prompts/` |
| **Slash commands** | `/dewey [command]` | `/dewey [command]` | `/dewey [command]` |

---

### spec-7-slash-commands.md

**Universal Slash Command Interface**

**Requirements**:
1. Slash command registration for each provider
2. Unified command interface across providers
3. Extension packaging (Claude Code extensions format)
4. Command discovery and help system
5. Interactive prompts for user input when needed
6. Progress indicators for long-running operations
7. Error handling with helpful messages

**Core Slash Commands**:

| Command | Description | Example |
|---------|-------------|---------|
| `/dewey init` | Initialize Dewey in repository | `/dewey init --provider=auto` |
| `/dewey check` | Run KPI evaluation, show metrics | `/dewey check` |
| `/dewey report` | Generate optimization recommendations | `/dewey report --format=md` |
| `/dewey fix [issue]` | Apply specific recommendation | `/dewey fix split-large-files` |
| `/dewey status` | Show current context health | `/dewey status --verbose` |
| `/dewey analyze [file]` | Analyze specific file | `/dewey analyze context/skills/ralph-loop-guide.md` |
| `/dewey session start` | Start session tracking | `/dewey session start research` |
| `/dewey session end` | End session, capture learnings | `/dewey session end` |
| `/dewey pr` | Generate optimization PR | `/dewey pr --target=main` |
| `/dewey help` | Show available commands | `/dewey help` |

**Acceptance Criteria**:
- All commands work identically across providers (Claude, Codex, Gemini)
- Commands registered automatically on extension install
- `/dewey help` shows provider-specific guidance
- Interactive prompts use provider's native input mechanisms
- Progress indicators show for operations >2 seconds
- Error messages are actionable ("File not found: run `/dewey init` first")
- Commands output markdown for easy reading
- Exit codes: 0 (success), 1 (user error), 2 (system error)

**Extension Installation Format**:

**Claude Code** (`.claude/extensions/dewey/`):
```yaml
# manifest.yaml
name: dewey
version: 1.0.0
description: Universal context optimization
author: Your Name
commands:
  - name: dewey
    description: Context optimization commands
    script: ./dewey.py
permissions:
  - read: context/**
  - write: .dewey/**
  - git: true
```

**OpenAI Codex** (`.codex/extensions/dewey/`):
```json
{
  "name": "dewey",
  "version": "1.0.0",
  "type": "command",
  "entry": "dewey.py",
  "commands": {
    "dewey": {
      "description": "Context optimization",
      "subcommands": ["init", "check", "report", "fix", "status"]
    }
  }
}
```

**Google Gemini** (`.gemini/plugins/dewey/`):
```yaml
# plugin.yaml
name: dewey
version: 1.0.0
type: tool
runtime: python
entry_point: dewey:main
capabilities:
  - file_read
  - file_write
  - git_operations
commands:
  dewey:
    description: Context optimization toolkit
    usage: /dewey [command] [options]
```

**Installation Flow**:

```bash
# Claude Code
claude code install dewey
# Downloads from registry, installs to .claude/extensions/dewey/
# Registers /dewey commands

# OpenAI Codex
codex extensions add dewey
# Installs to .codex/extensions/dewey/
# Registers /dewey commands

# Google Gemini
gemini plugins install dewey
# Installs to .gemini/plugins/dewey/
# Registers /dewey commands

# Manual installation (any provider)
git clone https://github.com/yourusername/dewey
cd dewey
python setup.py install
dewey register  # Auto-detects provider and installs
```

**Command Implementation Pattern**:

```python
# dewey/cli/commands.py

from dewey.providers import detect_provider

@command
def check(ctx):
    """Run KPI evaluation and show metrics"""
    provider = detect_provider(ctx.repo_path)
    metrics = provider.analyze_context()

    # Format output as markdown (works in all providers)
    output = f"""
# Dewey Context Health Check

## KPI Summary
- Context Utilization: {metrics.utilization:.0%}
- Token Efficiency: {metrics.efficiency:.2f}
- Files >500 lines: {metrics.large_files}

## Status
{'✅ HEALTHY' if metrics.is_healthy else '⚠️  NEEDS ATTENTION'}

Run `/dewey report` for optimization suggestions.
"""

    ctx.print_markdown(output)
    return 0 if metrics.is_healthy else 1

@command
def init(ctx, provider='auto'):
    """Initialize Dewey in repository"""
    if provider == 'auto':
        provider = detect_provider(ctx.repo_path)

    ctx.print(f"Initializing Dewey for {provider}...")

    # Install provider-specific files
    provider.install_templates(ctx.repo_path)

    ctx.print_success("""
✅ Dewey initialized!

Next steps:
1. Run `/dewey check` to see baseline metrics
2. Review `.dewey/config.yml` for customization
3. Commit changes to enable CI/CD Loop 2
""")

    return 0
```

---

## AGENTS.md

```markdown
# Dewey Build Commands

## Setup
```bash
cd dewey
python -m venv venv
source venv/bin/activate
pip install -e .
```

## Testing
```bash
pytest tests/ -v                    # All tests
pytest tests/test_measurement.py    # Measurement tests
pytest tests/test_compaction.py     # Compaction tests
pytest tests/test_commands/         # Slash command tests
```

## Linting
```bash
ruff check src/                     # Fast linter
mypy src/                           # Type checking
```

## Run Scripts
```bash
# Token inventory
python scripts/analyze-usage.py --inventory

# Weekly analysis
python scripts/analyze-usage.py --weekly

# File compaction
bash scripts/consolidate.sh

# Dashboard generation
python src/analytics/dashboard_generator.py

# Generate recommendations
python src/reports/generate_recommendations.py

# CI/CD checks
bash scripts/check_quality_gates.sh
```

## Extension Development
```bash
# Build provider-specific extensions
bash scripts/build-extensions.sh

# Test slash commands locally
python -m dewey.cli init --dry-run
python -m dewey.cli check --repo=/path/to/test/repo
python -m dewey.cli report --format=md

# Test in actual provider CLIs
# Claude Code: Install locally and test /dewey commands
claude code install ./src/extensions/claude

# Codex: Test command registration
codex extensions add ./src/extensions/codex

# Gemini: Test plugin installation
gemini plugins install ./src/extensions/gemini
```

## CI/CD Integration
```bash
# Loop 1: Dewey self-improvement (runs on Dewey repo)
.github/workflows/dewey-ci.yml

# Loop 2: Target repo checks (installs when user runs `dewey install`)
# Generates PR with recommendations
.github/workflows/dewey-checks.yml

# Pre-commit hook (target repo)
.git/hooks/pre-commit
```

## Installation

### Via Provider Extension Systems (Recommended)

```bash
# Claude Code
claude code install dewey
# Now available: /dewey commands

# OpenAI Codex
codex extensions add dewey
# Now available: /dewey commands

# Google Gemini
gemini plugins install dewey
# Now available: /dewey commands
```

### Via pip + Manual Registration

```bash
# Install Dewey globally
pip install dewey-context-optimizer

# Register with your provider
cd /path/to/your/repo
dewey register  # Auto-detects provider and installs extension

# Or manually specify provider
dewey register --provider=claude
```

### Initialize in Repository

```bash
# In your project directory
cd /path/to/your/repo

# Using slash command (if provider CLI is active)
/dewey init

# Or via direct command
dewey init  # Auto-detects provider

# Installs:
# - .dewey/config.yml
# - .github/workflows/dewey-checks.yml (Loop 2)
# - Pre-commit hooks
# - Provider-specific templates
```

## Integration Tests
```bash
# End-to-end workflow
bash tests/integration/test_workflow.sh

# CI/CD quality gates
bash tests/integration/test_quality_gates.sh
```
```

---

## PROMPT_plan.md

```markdown
# Context Curator - Planning Mode

## 0a-0d: Orientation Phase

Study the following using up to 500 parallel Sonnet subagents:

0a. **Research documents** in `projects/context-system-optimization/`:
   - proposals/gemini.md (full MCP architecture)
   - research/signal-to-noise-measurement.md (metrics)
   - research/compaction-strategies.md (summarization)
   - analysis/empirical-solutions.md (implementation methods)

0b. **Specifications** in `specs/`:
   - spec-1-measurement.md
   - spec-2-compaction.md
   - spec-3-memory-tier.md
   - spec-4-analytics.md
   - spec-5-mcp-server.md

0c. **Existing context system** in `context/`:
   - Current structure (148 files, 18 domains)
   - context-system/ domain files
   - Index and loading map

0d. **Review @IMPLEMENTATION_PLAN.md** (if exists)

## 1: Gap Analysis

Compare specifications against current implementation:

1. **DON'T implement** - planning only
2. **DON'T assume missing** - confirm via code search first
3. **Using parallel subagents** for discovery

For each spec:
- What exists already?
- What's missing?
- What needs modification?
- Dependencies between components?

## 2: Prioritize Tasks

Create ordered task list in @IMPLEMENTATION_PLAN.md:

**Prioritization criteria**:
1. Dependencies first (foundational components)
2. Quick wins (high value, low effort)
3. Measurable impact (clear before/after metrics)
4. Risk mitigation (security, data integrity)

**Task format**:
```
[ ] Task: Brief description
    Spec: spec-X-name.md
    Dependencies: [task IDs if any]
    Acceptance: Specific pass/fail criteria
    Estimate: S/M/L complexity
```

## 3: Phase Definition

Organize tasks into phases:

**Phase 0: Quick Start** (Week 1 - Foundation)
- Context inventory script (token counting)
- Session tracking template
- Baseline measurements
- One quick win (split largest file OR archive old files)

**Phase 1: Measurement** (Weeks 2-4 - Analytics Foundation)
- Token counting and frequency tracking
- CSV logging infrastructure
- Basic analysis scripts
- First weekly dashboard
- Citation tracking (grep-based)

**Phase 2: Optimization** (Month 2 - Compaction + Memory Tier)
- File splitting (automated)
- Duplicate detection
- Extractive summarization
- Session management
- Promotion workflow
- Time-based archival

**Phase 3: Automation & Extensions** (Month 3+ - Dual CI/CD + Multi-Provider + Slash Commands)
- Dual CI/CD loops (self-improvement + target repo)
- Transparent PR generation (both loops)
- KPI evaluation script
- Quality gates (file size, duplicates, dead links)
- Recommendation engine
- Pre-commit hooks
- GitHub Actions integration
- Multi-provider adapters (Claude, Codex, Gemini)
- Provider auto-detection
- **Slash command interface** (`/dewey [command]`)
- **Extension packaging** (Claude Code, Codex, Gemini formats)
- **Extension installation** via provider registries
- Command registration and discovery
- Interactive prompts and progress indicators
- A/B testing framework

## 4: Update and Exit

1. Write comprehensive @IMPLEMENTATION_PLAN.md
2. Commit with message: "plan: context curator implementation plan"
3. **Exit** - do NOT implement anything

## 999+: Planning Guardrails (High Priority)

- **Study** codebase, don't just "read"
- **Don't assume not implemented** - search first
- **No implementation** during planning
- **Regenerate plan** if trajectory diverges
- **Trust existing patterns** in context system
```

---

## PROMPT_build.md

```markdown
# Context Curator - Building Mode

## 0a-0c: Orientation Phase

0a. **Study specifications** in `specs/` using up to 500 parallel Sonnet subagents
0b. **Review @IMPLEMENTATION_PLAN.md** - what's done, what's next
0c. **Study application source** in `.claude/context-curator/src/`

## 1: Choose Task

Select **EXACTLY ONE** task from @IMPLEMENTATION_PLAN.md:

- **Most important** incomplete task (respecting dependencies)
- **Search existing code** before making changes (don't assume not implemented)
- **Up to 500 Sonnet subagents** for reads/searches
- **Only 1 subagent** for build/tests (backpressure control)

## 2: Implement Functionality

Build complete, production-ready implementation:

**Code Quality**:
- Type hints on all functions (mypy compliant)
- Docstrings (Google style)
- Error handling with informative messages
- No placeholders or TODOs
- Follow PEP 8 conventions

**Test Coverage**:
- Unit tests for core logic
- Integration tests for workflows
- Edge cases and error conditions
- Achieve >80% coverage for new code

**Security** (if applicable):
- Input validation
- Path traversal protection
- PII scrubbing
- Fail-closed on errors

## 3: Validate

Run complete test suite using **only 1 subagent**:

```bash
# Type checking
mypy src/

# Linting
ruff check src/

# Tests
pytest tests/ -v --cov=src

# Integration
bash tests/integration/test_workflow.sh
```

**All checks must pass** before proceeding.

## 4: Update and Commit

1. Mark task complete in @IMPLEMENTATION_PLAN.md
2. Update related documentation if needed
3. **Capture the "why" in commits** (not just "what")

```bash
git add .
git commit -m "feat(component): brief description

Why: Explain rationale and design decisions
Context: Reference spec and acceptance criteria
Tests: Confirm all validations passing"
```

## 999+: Building Guardrails (High Priority)

### Context Management
- **One task per iteration** - strict rule
- **Study** codebase with subagents, don't "read" in main context
- **Fresh start** each iteration (exit after task completion)
- **Trust code patterns** over exhaustive instructions

### Implementation Standards
- **No placeholders** - complete functionality only
- **No stubs** - fully implemented or not at all
- **Tests required** - derived from acceptance criteria
- **Security first** - validate inputs, scrub PII, fail closed

### Completion Detection
- **Don't assume done** - verify via tests
- **Backpressure works** - tests/linting enforce quality
- **Ultrathink before** marking complete
- **When truly done**, output: `<promise>ALL TESTS PASSING</promise>`

### Anti-Patterns
- ❌ Multiple tasks per iteration (context bloat)
- ❌ Verbose progress notes in AGENTS.md (keep <60 lines)
- ❌ Skipping deterministic validation
- ❌ Fighting with stale plan (regenerate instead)
- ❌ Assuming implementation without searching
```

---

## Completion Promise

**When all specs implemented and tests passing**, output:

```
<promise>DEWEY UNIVERSAL CONTEXT OPTIMIZER COMPLETE</promise>
```

**Verification checklist**:
- [ ] All 8 specs have complete implementations (Quick Start through Slash Commands)
- [ ] Test coverage >80% (pytest --cov)
- [ ] All type checks pass (mypy)
- [ ] All lint checks pass (ruff)
- [ ] Integration tests pass for all 3 providers (Claude, Codex, Gemini)
- [ ] Weekly dashboard generates successfully
- [ ] Dual CI/CD loops working (self-improvement + target repo)
- [ ] Both loops generate transparent PRs (not direct commits)
- [ ] Recommendation engine produces actionable suggestions
- [ ] Pre-commit hooks installed and functional
- [ ] Provider auto-detection works correctly
- [ ] Installation via provider extensions working (Claude Code, Codex, Gemini)
- [ ] All slash commands work identically across providers
- [ ] `/dewey help` provides clear guidance
- [ ] Interactive prompts work in all provider CLIs
- [ ] Extension manifests valid for all providers
- [ ] `dewey register` auto-installs for detected provider
- [ ] Documentation complete (usage + architecture + provider differences + slash commands)

---

## Usage Instructions

### Step 1: Start Planning Mode

```bash
cd /Users/bbeidel/Documents/notes/projects/context-system-optimization
/ralph-loop --prompt PROMPT_plan.md --max-iterations 5
```

**Expected output**: Detailed `IMPLEMENTATION_PLAN.md` with phased tasks

### Step 2: Review Plan

Manually review the generated plan:
- Are tasks properly ordered?
- Are acceptance criteria clear?
- Are phases realistic?

Make adjustments if needed.

### Step 3: Start Building Mode

```bash
/ralph-loop --prompt PROMPT_build.md \
  --completion-promise "CONTEXT CURATOR PRODUCTION READY" \
  --max-iterations 50
```

**What happens**:
- Ralph builds one task per iteration
- Tests enforce quality (backpressure)
- Exits when all specs implemented and validated

### Step 4: Validate Results

After completion:

```bash
# Run full test suite
cd dewey
pytest tests/ -v --cov=src

# Test all providers
pytest tests/test_providers/ -v

# Test slash commands
pytest tests/test_commands/ -v

# Generate first dashboard
python scripts/analyze-usage.py --weekly

# Test Loop 1: Self-improvement detection
python -m dewey.cicd.self_improvement --dry-run

# Test Loop 2: Target repo recommendations
python -m dewey.cicd.target_repo_gates --dry-run --repo=/path/to/test/repo

# Validate provider detection
python -m dewey.providers.detect --repo=/path/to/test/repo

# Test extension installation
bash scripts/build-extensions.sh
# Then manually test in each provider CLI

# Test slash commands locally
python -m dewey.cli check --repo=/path/to/test/repo
python -m dewey.cli report --format=md
python -m dewey.cli help

# Install in test repository via extension
cd /path/to/test/repo
# Use provider-specific installation:
claude code install dewey  # Or: codex extensions add dewey

# Then test slash commands in provider CLI
/dewey check
/dewey report
/dewey status

# Verify full workflow
/dewey init
/dewey check
/dewey report
/dewey pr  # Generate optimization PR
```

---

## Research References

All implementation should reference these comprehensive research documents:

1. **Gemini Architecture** (`proposals/gemini.md`):
   - Context Relevance Score algorithm (CRS = 0.6×semantic + 0.2×recency + 0.2×graph) - **for reference only, not required for local system**
   - Progressive disclosure concepts (metadata → detail hierarchy)
   - Security requirements (PII scrubbing, path validation, boundary enforcement)
   - **Note**: MCP server architecture in this document is optional future enhancement, NOT required for core system

2. **Signal-to-Noise Research** (`research/signal-to-noise-measurement.md`):
   - Production metrics (context utilization, token efficiency, information density)
   - RAG evaluation (Precision@k, Recall@k, Context Relevance, NDCG)
   - Compression techniques (relevance filtering, semantic summarization, hard pruning)
   - Observability tools (LangSmith, LangFuse, Weights & Biases Weave)
   - Benchmarks (context utilization 20-40%, token efficiency >0.70, info density >0.70)

3. **Compaction Strategies** (`research/compaction-strategies.md`):
   - Anthropic's implementation (token thresholds, what to preserve/discard)
   - RAPTOR hierarchical summarization (recursive clustering and summarization)
   - Production memory systems (multi-tier architecture, 91% latency improvement)
   - Pruning strategies (time-based decay, salience, LRU, feedback-weighted)
   - Decision framework (file-level, domain-level, system-level criteria)

4. **Mid-Term Memory Tier** (`research/mid-term-memory-tier.md`):
   - Memory type taxonomy (working, episodic, semantic)
   - Production architectures (MIRIX 6-component system, Memoria hot/cold)
   - Session management patterns (buffer window, summary buffer, hierarchical)
   - Promotion criteria (importance signals, scope-based persistence, triggers)
   - 7-day retention sweet spot (industry standard)

5. **Usage Analytics** (`research/usage-analytics.md`):
   - Context effectiveness metrics (utilization, quality, performance, UX)
   - Context attribution & tracing (citation tracking, attention analysis, ablation)
   - A/B testing methodology (hypothesis, variants, statistical significance)
   - Production platforms (Langfuse, Datadog, PromptLayer, Helicone, WhyLabs)
   - Feedback loops (measure → analyze → hypothesize → test → deploy)

6. **Empirical Solutions** (`analysis/empirical-solutions.md`):
   - Non-LLM methods for all 4 gaps
   - Token counting (4 chars ≈ 1 token)
   - Frequency tracking via CSV
   - Citation detection via grep
   - Extractive summarization (TF-IDF, no LLM required)
   - Implementation roadmap (4 phases)

7. **Quick Start Guide** (`analysis/quick-start-guide.md`):
   - "Start Small" philosophy
   - 30-minute quick wins (inventory, session template, file splitting)
   - Weekly optimization loop
   - Success metrics and common pitfalls

---

## Expected Outcomes

### Quantitative Improvements

**Signal-to-Noise** (from research benchmarks):
- Context Utilization: Unknown → 30-50% (target), 20-40% (excellent)
- Token Efficiency: Unknown → >0.70 (target), >0.80 (excellent)
- Information Density: Unknown → >0.70 (target), >0.85 (excellent)
- Precision@5: Unknown → >0.70 (target), >0.85 (excellent)
- Context Reference Rate: Unknown → >70% (excellent)

**Compaction** (based on production systems):
- Files >500 lines: 2+ → 0
- Duplicate content: Unknown → <5%
- Compression ratio: 5:1 to 10:1 (with 90-95% retention)
- Token waste reduction: 20-40%
- Latency improvement: 30-50% (from Mem0: 91% p95 reduction)

**Memory Tier** (industry standards):
- Promotion lag: 0 days → 7 days (staged)
- False promotions: Unknown → <10%
- Capture rate: >80% learnings captured
- Promotion precision: >90% promotions valuable
- Review time: <20 min/week

**Analytics** (observability metrics):
- Data coverage: 0% → >90% of sessions
- Weekly dashboards: 0 → 1 per week
- A/B test capability: No → Yes

### Qualitative Improvements

**Data-Driven Decisions**:
- From: "I think this file is useful"
- To: "Data shows 85% citation rate"

**Continuous Optimization**:
- From: One-time setup
- To: Weekly measurement and improvement

**Lower Maintenance**:
- From: Manual, ad-hoc
- To: Automated, scheduled

---

## Success Metrics

Ralph Loop succeeds when:

1. **Foundation Working** (Phase 1):
   - CSV logging captures >90% of sessions
   - Weekly dashboard generates automatically
   - Token inventory shows complete picture

2. **Optimization Active** (Phase 2):
   - All files <500 lines
   - Duplicates identified and consolidated
   - Mid-term memory tier in use

3. **Automation Deployed** (Phase 3):
   - Dual CI/CD loops operational (self-improvement + target repo)
   - Both loops generate transparent PRs for human review
   - Multi-provider support (Claude, Codex, Gemini)
   - Provider auto-detection working
   - Pre-commit hooks prevent regressions
   - `dewey init` installs successfully in target repos

4. **Measurable Impact**:
   - Context utilization improves by 20%+
   - Token efficiency increases by 15%+
   - Time to relevant context reduces by 30%+
   - Transparent recommendations drive continuous improvement
   - Works across all major LLM CLI providers
   - Loop 1: Dewey continuously self-improves based on usage patterns
   - Loop 2: Target repos get regular optimization suggestions via PR